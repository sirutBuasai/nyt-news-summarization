{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvoGHJJvOyAe"
      },
      "source": [
        "# New York Times News Summarization\n",
        "Sirut Buasai, sbausai2@wpi.edu <br>\n",
        "Jason Dykstra, jpdykstra@wpi.edu <br>\n",
        "Adam Yang ayang@wpi.edu\n",
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "trusted": true,
        "id": "emBF0izoOyAo"
      },
      "outputs": [],
      "source": [
        "# Data Processing\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Text Processing\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import pos_tag_sents\n",
        "import spacy\n",
        "NER = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Model Building\n",
        "import tensorflow as tf\n",
        "from keras.utils import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEs_PaTROyAs"
      },
      "source": [
        "## Data Pre-processing\n",
        "### Initial Data Inspection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEDTd5yNOyAu"
      },
      "outputs": [],
      "source": [
        "raw_data_path = 'NYT_Dataset.csv'\n",
        "raw_data = pd.read_csv(raw_data_path)\n",
        "print(f'dataframe shape: {raw_data.shape}')\n",
        "raw_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yP0xgBIqOyAv"
      },
      "source": [
        "### Clean Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FUNCTION FOR PARTS-OF-SPEECH PROCESSING\n",
        "def pos_processing(abstract_tokens):\n",
        "    # identify parts-of-speech; pos_tag requires list of tokens\n",
        "    abstract_pos = pd.Series(pos_tag_sents(abstract_tokens))\n",
        "    \n",
        "    # create attention mask(s) using parts-of-speech\n",
        "    nounsList = ['NN', 'NNS', 'NNP', 'NNPS'] # nouns to pay attention to\n",
        "    noun_attention_mask = abstract_pos.apply(lambda words: [1 if pair[1] in nounsList else 0 for pair in words])\n",
        "\n",
        "    return noun_attention_mask"
      ],
      "metadata": {
        "id": "lt2rCg-K7bn-"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FUNCTIONS FOR NER PROCESSING\n",
        "def create_ne_attn_mask(entities):\n",
        "    # extract named entities\n",
        "    named_entities = entities.ents\n",
        "    # initialize attention mask\n",
        "    attn_mask = [0 for i in range(len(entities))]\n",
        "\n",
        "    # assign 1's to positions in attn mask corresponding to named-entity positions\n",
        "    for entity in named_entities:\n",
        "        attn_mask[entity.start: entity.end] = [1 for i in range(entity.start,entity.end)]\n",
        "\n",
        "    return attn_mask\n",
        "\n",
        "\n",
        "def ner_processing(abstract_sentences):\n",
        "    # identify entities for each abstract\n",
        "    # NOTE: this is supposedly faster if we use nlp.pipe, but we can look into that tomorrow ...\n",
        "    abs_entities = pd.Series(abstract_sentences.apply(lambda sentence: NER(sentence)))\n",
        "\n",
        "    # create attention mask(s) using the NE's\n",
        "    ne_attention_mask = abs_entities.apply(lambda entities: create_ne_attn_mask(entities))\n",
        "\n",
        "    return ne_attention_mask\n"
      ],
      "metadata": {
        "id": "8eN1mcbw8rbj"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "id": "dvJXzEeFOyAw"
      },
      "outputs": [],
      "source": [
        "# GENERAL DATA-PREPROCESSING FUNCTION\n",
        "def preprocess_data(df):\n",
        "    # drop unnecessary columns\n",
        "    unused_columns = ['Unnamed: 0', 'Date', 'ID', 'topic', 'keywords']\n",
        "    df.drop(unused_columns, axis=1, inplace=True, errors='ignore')\n",
        "\n",
        "    # drop dupliates and nan rows\n",
        "    df.drop_duplicates(subset=['abstract'], inplace=True)\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    df.dropna(axis=0, inplace=True)\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    # convert texts to lowercase\n",
        "    df['abstract'] = df['abstract'].str.lower()\n",
        "    df['title'] = df['title'].str.lower()\n",
        "\n",
        "    # strip special characters\n",
        "    df['cleaned_title'] = df['title'].apply(lambda string: re.sub(\"[^a-zA-Z0-9 ]+\", '', string))\n",
        "    df['cleaned_abstract'] = df['abstract'].apply(lambda string: re.sub(\"[^a-zA-Z0-9 ]+\", '', string))\n",
        "\n",
        "    # convert string to list\n",
        "    df['cleaned_title'] = df['cleaned_title'].apply(lambda string: string.split())\n",
        "    df['cleaned_abstract'] = df['cleaned_abstract'].apply(lambda string: string.split())\n",
        "\n",
        "    # remove stopwords\n",
        "    stop_words = stopwords.words('english')\n",
        "    df['cleaned_title'] = df['cleaned_title'].apply(lambda sentence: [word for word in sentence if word not in (stop_words)])\n",
        "    df['cleaned_abstract'] = df['cleaned_abstract'].apply(lambda sentence: [word for word in sentence if word not in (stop_words)])\n",
        "\n",
        "    # generate noun attention mask from parts-of-speech\n",
        "    df['noun_attn_mask'] = pos_processing(df['cleaned_abstract'])\n",
        "\n",
        "    # convert list back to string\n",
        "    df['cleaned_title'] = df['cleaned_title'].apply(lambda sentence: \" \".join(word for word in sentence))\n",
        "    df['cleaned_abstract'] = df['cleaned_abstract'].apply(lambda sentence: \" \".join(word for word in sentence))\n",
        "    \n",
        "    # identify named-entities; NER requires string sentences\n",
        "    df['ne_attention_mask'] = ner_processing(df['cleaned_abstract'])\n",
        "    \n",
        "    # add start and end tokens for title\n",
        "    df['cleaned_title'] = df['cleaned_title'].apply(lambda text: '_START_ ' + text + ' _END_')\n",
        "\n",
        "    return df\n",
        "\n",
        "cleaned_data = preprocess_data(raw_data)\n",
        "print(f'dataframe shape: {cleaned_data.shape}')\n",
        "cleaned_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9xy7VYsOyAy"
      },
      "source": [
        "### Declare Maximum Title and Abstract Length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSsfl3dDOyA0"
      },
      "outputs": [],
      "source": [
        "MAX_TITLE_LEN = 100\n",
        "MAX_ABSTRACT_LEN = 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jc7jseOdOyA2"
      },
      "source": [
        "### Split Training and Testing Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRIL1YDyOyA3"
      },
      "outputs": [],
      "source": [
        "abs_tr, abs_te, ttl_tr, ttl_te = train_test_split(\n",
        "  cleaned_data['cleaned_abstract'],\n",
        "  cleaned_data['cleaned_title'],\n",
        "  test_size=0.1,\n",
        "  random_state=0,\n",
        "  shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvUKlvl1OyA4"
      },
      "source": [
        "### Tokenize Title and Abstracts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3dOe-5fOyA5"
      },
      "outputs": [],
      "source": [
        "def tokenize_text(train_set, test_set):\n",
        "  # prepare a tokenizer for reviews on training data\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(list(train_set))\n",
        "\n",
        "  # convert text sequences into integer sequences\n",
        "  train_set = tokenizer.texts_to_sequences(train_set) \n",
        "  test_set = tokenizer.texts_to_sequences(test_set)\n",
        "\n",
        "  # padding zero up to maximum length\n",
        "  train_set = pad_sequences(train_set, maxlen=MAX_ABSTRACT_LEN, padding='post') \n",
        "  test_set = pad_sequences(test_set, maxlen=MAX_ABSTRACT_LEN, padding='post')\n",
        "\n",
        "  # get vector size\n",
        "  vocab_size = len(tokenizer.word_index)+1\n",
        "\n",
        "  return tokenizer, train_set, test_set, vocab_size\n",
        "\n",
        "abs_tokenizer, abs_tr, abs_te, abs_size = tokenize_text(abs_tr, abs_te)\n",
        "ttl_tokenizer, ttl_tr, ttl_te, ttl_size = tokenize_text(ttl_tr, ttl_te)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TaFaS9iOyA5"
      },
      "source": [
        "## Build LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yq1yrNpLOyA6"
      },
      "outputs": [],
      "source": [
        "from keras import backend as K \n",
        "K.clear_session() \n",
        "latent_dim = 500 \n",
        "\n",
        "# encoder \n",
        "encoder_inputs = tf.keras.layers.Input(shape=(MAX_ABSTRACT_LEN,)) \n",
        "enc_emb = tf.keras.layers.Embedding(abs_size, latent_dim,trainable=True)(encoder_inputs) \n",
        "\n",
        "# LSTM 1 \n",
        "encoder_lstm1 = tf.keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True) \n",
        "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb) \n",
        "\n",
        "# LSTM 2 \n",
        "encoder_lstm2 = tf.keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True) \n",
        "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
        "\n",
        "# LSTM 3 \n",
        "encoder_lstm3 = tf.keras.layers.LSTM(latent_dim, return_state=True, return_sequences=True) \n",
        "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2) \n",
        "\n",
        "# set up the decoder. \n",
        "decoder_inputs = tf.keras.layers.Input(shape=(None,)) \n",
        "dec_emb_layer = tf.keras.layers.Embedding(ttl_size, latent_dim, trainable=True) \n",
        "dec_emb = dec_emb_layer(decoder_inputs) \n",
        "\n",
        "# LSTM using encoder_states as initial state\n",
        "decoder_lstm = tf.keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True) \n",
        "decoder_outputs, decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c]) \n",
        "\n",
        "# dense layer\n",
        "decoder_dense = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(ttl_size, activation='softmax')) \n",
        "decoder_outputs = decoder_dense(decoder_outputs) \n",
        "\n",
        "# define the model\n",
        "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], decoder_outputs) \n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2E4k41J3OyA7"
      },
      "source": [
        "### Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTQyN3I7OyA8"
      },
      "outputs": [],
      "source": [
        "# initialize model\n",
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "# initialize early stopping when validation loss increases\n",
        "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
        "\n",
        "# train model\n",
        "history = model.fit([abs_tr, ttl_tr[:,:-1]], ttl_tr.reshape(ttl_tr.shape[0], ttl_tr.shape[1], 1)[:,1:],\n",
        "                    epochs=50,\n",
        "                    callbacks=[es],\n",
        "                    batch_size=512,\n",
        "                    validation_data=([abs_te,ttl_te[:,:-1]], ttl_te.reshape(ttl_te.shape[0], ttl_te.shape[1], 1)[:,1:]))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.14 64-bit ('3.9.14')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.14"
    },
    "vscode": {
      "interpreter": {
        "hash": "d50662f09ec6209d433ff0029bf9cc367be01420c8c5568a1e0b3ae42a6d5264"
      }
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}