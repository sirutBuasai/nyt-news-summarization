{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvoGHJJvOyAe"
      },
      "source": [
        "# New York Times News Summarization\n",
        "Sirut Buasai, sbausai2@wpi.edu <br>\n",
        "Jason Dykstra, jpdykstra@wpi.edu <br>\n",
        "Adam Yang ayang@wpi.edu\n",
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "emBF0izoOyAo",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Data Processing\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import csv\n",
        "\n",
        "# Text Processing\n",
        "import nltk\n",
        "import re\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import pos_tag_sents\n",
        "\n",
        "# Model Building\n",
        "import tensorflow as tf\n",
        "from keras.utils import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Metrics\n",
        "!pip install sacrebleu\n",
        "!pip install rouge\n",
        "from sacrebleu import sentence_bleu\n",
        "from rouge import Rouge\n",
        "\n",
        "# Google Colab\n",
        "from google.colab import drive, files\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Downloads\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "NER = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B73993JWevBU"
      },
      "source": [
        "## Global Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntkhyPUaevBU"
      },
      "outputs": [],
      "source": [
        "MAX_ABSTRACT_LEN = 30\n",
        "MAX_TITLE_LEN = 12\n",
        "LATENT_DIMENSION = 300 \n",
        "EMBEDDING_DIMENSION = 100 \n",
        "EPOCH = 10\n",
        "BATCH_SIZE = 128\n",
        "DATA_SET_RATIO = 0.80\n",
        "MODEL_CONDITION = \"cleaned_abstract\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEs_PaTROyAs"
      },
      "source": [
        "## Data Pre-processing\n",
        "### Initial Data Inspection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEDTd5yNOyAu"
      },
      "outputs": [],
      "source": [
        "# download and make initial inspection of the dataset\n",
        "raw_data_path = '/content/gdrive/My Drive/Fourth Year/Natural Language Processing/Final Project/NYT_Dataset.csv'\n",
        "raw_data = pd.read_csv(raw_data_path)\n",
        "raw_data = raw_data[:int(DATA_SET_RATIO*len(raw_data))]\n",
        "print(f'dataframe shape: {raw_data.shape}')\n",
        "raw_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yP0xgBIqOyAv"
      },
      "source": [
        "### Clean Data\n",
        "#### Part of Speech Processing Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lt2rCg-K7bn-"
      },
      "outputs": [],
      "source": [
        "# Parts-of-Speech processing to filter and keep certain POS words\n",
        "def pos_processing(abstract_sentences):\n",
        "    # split the sentence into tokens\n",
        "    abstract_tokens = pd.Series(abstract_sentences.apply(lambda sentence: sentence.split()))\n",
        "\n",
        "    # identify parts-of-speech; pos_tag requires list of tokens\n",
        "    abstract_pos = pd.Series(pos_tag_sents(abstract_tokens))\n",
        "\n",
        "    # filter non-desirable parts-of-speech\n",
        "    nounsList = ['NN', 'NNS', 'NNP', 'NNPS'] # Nouns\n",
        "    verbsList = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VPZ'] # Verbs\n",
        "    adjList = ['JJ', 'JJR', 'JJS'] # Adjectives\n",
        "    pos_list = nounsList + verbsList + adjList\n",
        "    abstract_pos_filtered = abstract_pos.apply(lambda words: ' '.join([pair[0] for pair in words if pair[1] in pos_list]))\n",
        "\n",
        "    return abstract_pos_filtered"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRRQSO8eevBY"
      },
      "source": [
        "#### NER Processing Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eN1mcbw8rbj"
      },
      "outputs": [],
      "source": [
        "# NER function to identify and embed NER tags\n",
        "def ner_processing(abstract_sentences):\n",
        "  # identify entities for each abstract\n",
        "  abs_entities = pd.Series(abstract_sentences.apply(lambda sentence: NER(sentence)))\n",
        "\n",
        "  # extract the original sentences, but now with split contractions and the entity type from NER\n",
        "  original_sentence = pd.Series(abs_entities.apply(lambda entities: ' '.join([f\"{token.orth_}<{token.ent_type_}>\" if token.ent_type_ else f\"{token.orth_}\" for token in entities])))\n",
        "\n",
        "  return original_sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPpI_mowevBZ"
      },
      "source": [
        "#### Clean Text Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "dvJXzEeFOyAw",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Main preprocessing function\n",
        "def preprocess_data(df):\n",
        "    # drop unnecessary columns\n",
        "    unused_columns = ['Unnamed: 0', 'Date', 'ID']\n",
        "    df.drop(unused_columns, axis=1, inplace=True, errors='ignore')\n",
        "\n",
        "    # drop duplicates and nan rows\n",
        "    df.drop_duplicates(subset=['abstract'], inplace=True)\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    df.dropna(axis=0, inplace=True)\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    # convert texts to lowercase\n",
        "    df['abstract'] = df['abstract'].str.lower()\n",
        "    df['title'] = df['title'].str.lower()\n",
        "    df['keywords'] = df['keywords'].str.lower()\n",
        "    df['topic'] = df['topic'].str.lower()\n",
        "\n",
        "    # strip special characters\n",
        "    df['cleaned_title'] = df['title'].apply(lambda text: re.sub(\"[^a-zA-Z0-9 ]+\", '', text))\n",
        "    df['cleaned_abstract'] = df['abstract'].apply(lambda text: re.sub(\"[^a-zA-Z0-9 ]+\", '', text))\n",
        "    df['cleaned_keywords'] = df['keywords'].apply(lambda text: re.sub(\"[^a-zA-Z0-9 ]+\", '', text))\n",
        "    df['cleaned_topic'] = df['topic'].apply(lambda text: re.sub(\"[^a-zA-Z0-9 ]+\", '', text))\n",
        "\n",
        "    # add keywords and topics\n",
        "    df['cleaned_keywords'] = df['cleaned_keywords'].apply(lambda sentence: \"\".join(word for word in sentence))\n",
        "    df['cleaned_abstract_keywords'] = df['cleaned_abstract'] + df['cleaned_keywords'] + ' ' + df['topic']\n",
        "\n",
        "    # convert string to list\n",
        "    df['cleaned_title'] = df['cleaned_title'].apply(lambda text: text.split())\n",
        "    df['cleaned_abstract'] = df['cleaned_abstract'].apply(lambda text: text.split())\n",
        "    df['cleaned_abstract_keywords'] = df['cleaned_abstract_keywords'].apply(lambda text: text.split())\n",
        "\n",
        "    # remove stopwords\n",
        "    stop_words = stopwords.words('english')\n",
        "    df['cleaned_title'] = df['cleaned_title'].apply(lambda sentence: [word for word in sentence if word not in (stop_words)])\n",
        "    df['cleaned_abstract'] = df['cleaned_abstract'].apply(lambda sentence: [word for word in sentence if word not in (stop_words)])\n",
        "    df['cleaned_abstract_keywords'] = df['cleaned_abstract_keywords'].apply(lambda sentence: [word for word in sentence if word not in (stop_words)])\n",
        "\n",
        "    # convert list back to string\n",
        "    df['cleaned_title'] = df['cleaned_title'].apply(lambda sentence: \" \".join(word for word in sentence))\n",
        "    df['cleaned_abstract'] = df['cleaned_abstract'].apply(lambda sentence: \" \".join(word for word in sentence))\n",
        "    df['cleaned_abstract_keywords'] = df['cleaned_abstract_keywords'].apply(lambda sentence: \" \".join(word for word in sentence))\n",
        "\n",
        "    # filter and keep noun, verbs, and adj words; requires string sentences\n",
        "    df['cleaned_abstract_pos'] = pos_processing(df['cleaned_abstract'])\n",
        "    df['cleaned_abstract_pos_keywords'] = pos_processing(df['cleaned_abstract_keywords'])\n",
        "\n",
        "    # identify named-entities; NER requires string sentences\n",
        "    df['cleaned_title'] = ner_processing(df['cleaned_title'])\n",
        "    df['cleaned_abstract_pos_ner'] = ner_processing(df['cleaned_abstract_pos'])\n",
        "    df['cleaned_abstract_pos_keywords_ner'] = ner_processing(df['cleaned_abstract_pos_keywords'])\n",
        "\n",
        "    # add start and end tokens for title\n",
        "    df['cleaned_title'] = df['cleaned_title'].apply(lambda text: 'sostok ' + text + ' eostok')\n",
        "\n",
        "    # drop unnecessary columns\n",
        "    unnecessary_columns = ['topic', 'keywords', 'cleaned_keywords', 'cleaned_topic', 'cleaned_abstract_keywords']\n",
        "    df.drop(unnecessary_columns, axis=1, inplace=True, errors='ignore')\n",
        "\n",
        "    # important columns are cleaned_title, cleaned_abstract, cleaned_abstract_pos_ner, cleaned_abstract_pos_keywords_ner\n",
        "    return df\n",
        "\n",
        "cleaned_data = preprocess_data(raw_data)\n",
        "# cleaned_data = pd.read_csv(\"cleaned_data.csv\")\n",
        "# print(f'dataframe shape: {cleaned_data.shape}')\n",
        "# cleaned_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SAVE DF TO DRIVE\n",
        "cleaned_data.to_pickle('/content/gdrive/My Drive/Fourth Year/Natural Language Processing/Final Project/NYT_preprocessed.pkl')\n",
        "\n",
        "# LOAD DF FROM DRIVE\n",
        "# cleaned_data = pd.read_pickle('/content/gdrive/My Drive/Fourth Year/Natural Language Processing/Final Project/NYT_preprocessed.pkl')  "
      ],
      "metadata": {
        "id": "610lhERJwQvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-T3iCuoHz6MA"
      },
      "outputs": [],
      "source": [
        "# print first five rows to check results\n",
        "for i in range(5):\n",
        "  print(\"Abstract:\\n\\t\", cleaned_data['abstract'][i])\n",
        "  print(\"Title:\\n\\t\", cleaned_data['title'][i])\n",
        "  print(\"\\n\")\n",
        "  print(\"Cleaned Abstract:\\n\\t\", cleaned_data['cleaned_abstract'][i])\n",
        "  print(\"Cleaned Abstract with POS:\\n\\t\", cleaned_data['cleaned_abstract_pos'][i])\n",
        "  print(\"Cleaned Abstract with POS and Keywords:\\n\\t\", cleaned_data['cleaned_abstract_pos_keywords'][i])\n",
        "  print(\"Cleaned Abstract with POS and NER:\\n\\t\", cleaned_data['cleaned_abstract_pos_ner'][i])\n",
        "  print(\"Cleaned Abstract with POS and Keywords and NER:\\n\\t\", cleaned_data['cleaned_abstract_pos_keywords_ner'][i])\n",
        "  print(\"Cleaned Title:\\n\\t\", cleaned_data['cleaned_title'][i])\n",
        "  print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUVwq9rjz6MB"
      },
      "source": [
        "### Insepct String Length Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TR7-xku0z6MC"
      },
      "outputs": [],
      "source": [
        "abstract_word_count = []\n",
        "title_word_count = []\n",
        "\n",
        "\n",
        "# Convert cleaned_abstract to all strings\n",
        "cleaned_data['cleaned_abstract'] = cleaned_data['cleaned_abstract'].astype(str)\n",
        "cleaned_data['cleaned_abstract_pos'] = cleaned_data['cleaned_abstract_pos'].astype(str)\n",
        "\n",
        "\n",
        "# populate the lists with sentence lengths\n",
        "for i in cleaned_data['cleaned_abstract']:\n",
        "  abstract_word_count.append(len(i.split()))\n",
        "\n",
        "for i in cleaned_data['cleaned_title']:\n",
        "  title_word_count.append(len(i.split()))\n",
        "\n",
        "length_df = pd.DataFrame({'abstract':abstract_word_count, 'title':title_word_count})\n",
        "\n",
        "length_df.hist(bins = 30)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jc7jseOdOyA2"
      },
      "source": [
        "### Split Training and Testing Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRIL1YDyOyA3"
      },
      "outputs": [],
      "source": [
        "# split the dataset into training and testing sets\n",
        "abs_tr, abs_te, ttl_tr, ttl_te = train_test_split(\n",
        "  np.array(cleaned_data[MODEL_CONDITION]),\n",
        "  np.array(cleaned_data['cleaned_title']),\n",
        "  test_size=0.1,\n",
        "  shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvUKlvl1OyA4"
      },
      "source": [
        "### Tokenize Title and Abstracts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3dOe-5fOyA5"
      },
      "outputs": [],
      "source": [
        "# Function to tokenize the datasets\n",
        "def tokenize_text(train_set, test_set, string_length, rare_word_threshold):\n",
        "  # find the number of rarewords\n",
        "  tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_{|}~\\t\\n') # filters prevent NER tag to be separated\n",
        "  tokenizer.fit_on_texts(list(train_set))\n",
        "  rare_word_count = 0\n",
        "  total_word_count = 0\n",
        "  for _,freq in tokenizer.word_counts.items():\n",
        "    total_word_count = total_word_count + 1\n",
        "    if (freq < rare_word_threshold):\n",
        "      rare_word_count += 1\n",
        "    \n",
        "  # prepare a tokenizer for reviews on training data\n",
        "  tokenizer = Tokenizer(num_words=total_word_count-rare_word_count, filters='!\"#$%&()*+,-./:;=?@[\\\\]^_{|}~\\t\\n') # filters prevent NER tag to be separated\n",
        "  tokenizer.fit_on_texts(list(train_set))\n",
        "\n",
        "  # convert text sequences into integer sequences\n",
        "  train_set = tokenizer.texts_to_sequences(train_set) \n",
        "  test_set = tokenizer.texts_to_sequences(test_set)\n",
        "\n",
        "  # padding zero up to maximum length\n",
        "  train_set = pad_sequences(train_set, maxlen=string_length, padding='post') \n",
        "  test_set = pad_sequences(test_set, maxlen=string_length, padding='post')\n",
        "\n",
        "  # get vector size\n",
        "  vocab_size = tokenizer.num_words+1\n",
        "\n",
        "  return tokenizer, train_set, test_set, vocab_size\n",
        "\n",
        "abs_tokenizer, abs_tr, abs_te, abs_size = tokenize_text(abs_tr, abs_te, MAX_ABSTRACT_LEN, 3)\n",
        "ttl_tokenizer, ttl_tr, ttl_te, ttl_size = tokenize_text(ttl_tr, ttl_te, MAX_TITLE_LEN, 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pe2geEa1z6MF"
      },
      "source": [
        "### Remove Empty Texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMv2K_Hmz6MG",
        "outputId": "4c3e01f5-a5ef-4cf4-d952-1c77e2082507"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "removed 205 empty texts.\n",
            "removed 0 empty texts.\n"
          ]
        }
      ],
      "source": [
        "# Function for removing strings with no text\n",
        "def remove_empty_text(abstract, title):\n",
        "    indices = []\n",
        "    for i in range(len(ttl_tr)):\n",
        "        cnt = 0\n",
        "        for j in ttl_tr[i]:\n",
        "            if j!=0:\n",
        "                cnt=cnt+1\n",
        "        if (cnt == 2):\n",
        "            indices.append(i)\n",
        "\n",
        "    abstract = np.delete(abs_tr, indices, axis=0)\n",
        "    title = np.delete(ttl_tr, indices, axis=0)\n",
        "    print(f\"removed {len(indices)} empty texts.\")\n",
        "\n",
        "    return abstract, title\n",
        "\n",
        "abs_tr, ttl_tr = remove_empty_text(abs_tr, ttl_tr)\n",
        "abs_te, ttl_te = remove_empty_text(abs_te, ttl_te)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TaFaS9iOyA5"
      },
      "source": [
        "## Build LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xO7ewBahz6MH"
      },
      "outputs": [],
      "source": [
        "tf.keras.backend.clear_session() \n",
        "\n",
        "# input layer\n",
        "encoder_inputs = tf.keras.layers.Input(shape=(MAX_ABSTRACT_LEN,))\n",
        "\n",
        "# embedding layer\n",
        "enc_emb =  tf.keras.layers.Embedding(abs_size, \n",
        "                                     EMBEDDING_DIMENSION, \n",
        "                                     trainable=True)(encoder_inputs)\n",
        "\n",
        "# encoder lstm 1\n",
        "encoder_lstm1 = tf.keras.layers.LSTM(LATENT_DIMENSION, \n",
        "                                     return_sequences=True, \n",
        "                                     return_state=True, \n",
        "                                     dropout=0.4, \n",
        "                                     recurrent_dropout=0.4)\n",
        "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
        "\n",
        "# encoder lstm 2\n",
        "encoder_lstm2 = tf.keras.layers.LSTM(LATENT_DIMENSION,\n",
        "                                     return_sequences=True,\n",
        "                                     return_state=True,\n",
        "                                     dropout=0.4,\n",
        "                                     recurrent_dropout=0.4)\n",
        "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
        "\n",
        "# encoder lstm 3\n",
        "encoder_lstm3 = tf.keras.layers.LSTM(LATENT_DIMENSION, \n",
        "                                     return_state=True, \n",
        "                                     return_sequences=True,\n",
        "                                     dropout=0.4,\n",
        "                                     recurrent_dropout=0.4)\n",
        "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
        "\n",
        "# set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = tf.keras.layers.Input(shape=(None,))\n",
        "\n",
        "# embedding layer\n",
        "dec_emb_layer = tf.keras.layers.Embedding(ttl_size, \n",
        "                                          EMBEDDING_DIMENSION, \n",
        "                                          trainable=True)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# decoder lstm\n",
        "decoder_lstm = tf.keras.layers.LSTM(LATENT_DIMENSION, \n",
        "                                    return_sequences=True, \n",
        "                                    return_state=True,\n",
        "                                    dropout=0.4,\n",
        "                                    recurrent_dropout=0.2)\n",
        "decoder_outputs, decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,\n",
        "                                                                     initial_state=[state_h, state_c])\n",
        "\n",
        "# attention layer\n",
        "attn_layer = tf.keras.layers.AdditiveAttention()\n",
        "attn_out = attn_layer([decoder_outputs, encoder_outputs])\n",
        "\n",
        "# concat attention input and decoder LSTM output\n",
        "decoder_concat_input = tf.keras.layers.Concatenate()([decoder_outputs, attn_out])\n",
        "\n",
        "# dense layer\n",
        "decoder_dense = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(ttl_size, activation='softmax'))\n",
        "decoder_outputs = decoder_dense(decoder_concat_input)\n",
        "\n",
        "# define the model \n",
        "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.summary() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2E4k41J3OyA7"
      },
      "source": [
        "### Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTQyN3I7OyA8"
      },
      "outputs": [],
      "source": [
        "# initialize model\n",
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "# train model\n",
        "history = model.fit([abs_tr, ttl_tr[:,:-1]], ttl_tr.reshape(ttl_tr.shape[0], ttl_tr.shape[1], 1)[:,1:],\n",
        "                    epochs=EPOCH,\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    validation_data=([abs_te,ttl_te[:,:-1]], ttl_te.reshape(ttl_te.shape[0], ttl_te.shape[1], 1)[:,1:]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7Lej6Mqz6MJ"
      },
      "source": [
        "### Inspect Training Validation Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lE4zW5Mz6MK"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Acs8_kXXz6ML"
      },
      "source": [
        "### Vec2Word Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tnzNDkVz6ML"
      },
      "outputs": [],
      "source": [
        "# grab the dictionaries to convert token ids back to string texts\n",
        "reverse_target_word_index = ttl_tokenizer.index_word \n",
        "reverse_source_word_index = abs_tokenizer.index_word \n",
        "target_word_index = ttl_tokenizer.word_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Dq56cHlz6ML"
      },
      "source": [
        "## Build Inference Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxSK6leDz6MM"
      },
      "outputs": [],
      "source": [
        "# encode the input sequence to get the feature vector\n",
        "encoder_model = tf.keras.models.Model(inputs=encoder_inputs,\n",
        "                                      outputs=[encoder_outputs, state_h, state_c])\n",
        "\n",
        "# decoder setup\n",
        "# below tensors will hold the states of the previous time step\n",
        "decoder_state_input_h = tf.keras.layers.Input(shape=(LATENT_DIMENSION,))\n",
        "decoder_state_input_c = tf.keras.layers.Input(shape=(LATENT_DIMENSION,))\n",
        "decoder_hidden_state_input = tf.keras.layers.Input(shape=(MAX_ABSTRACT_LEN, LATENT_DIMENSION))\n",
        "\n",
        "# get the embeddings of the decoder sequence\n",
        "dec_emb2= dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# to predict the next word in the sequence, set the initial states to the states from the previous time step\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
        "\n",
        "# attention inference\n",
        "attn_out_inf = attn_layer([decoder_outputs2, decoder_hidden_state_input])\n",
        "decoder_inf_concat = tf.keras.layers.Concatenate()([decoder_outputs2, attn_out_inf])\n",
        "\n",
        "# a dense softmax layer to generate prob dist. over the target vocabulary\n",
        "decoder_outputs2 = decoder_dense(decoder_inf_concat)\n",
        "\n",
        "# final decoder model\n",
        "decoder_model = tf.keras.models.Model([decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
        "                                      [decoder_outputs2] + [state_h2, state_c2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjtmZQRCz6MM"
      },
      "source": [
        "### Decode Inference Output Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjJMsmPNz6MM"
      },
      "outputs": [],
      "source": [
        "# Function to decode the output of the decoder model\n",
        "def decode_sequence(input_seq):\n",
        "    # encode the input as state vectors.\n",
        "    e_out, e_h, e_c = encoder_model.predict(input_seq, verbose=0)\n",
        "\n",
        "    # generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "\n",
        "    # populate the first word of target sequence with the start word.\n",
        "    target_seq[0, 0] = target_word_index['sostok']\n",
        "\n",
        "    # loop through the decoder model's outputs and build the output sentence\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        \n",
        "        # grab decoder model output tokens and internal states\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c], verbose=0)\n",
        "        \n",
        "        # convert token back to associated string text\n",
        "        try:\n",
        "          sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "          sampled_token = reverse_target_word_index[sampled_token_index]\n",
        "        except KeyError:\n",
        "          stop_condition = True\n",
        "\n",
        "        # remove NER tag if necessary\n",
        "        sampled_token = re.sub('<[^>]+>', '', sampled_token)\n",
        "\n",
        "        # if the token is not the end token, then add token to the output sentence\n",
        "        if (sampled_token != 'eostok'):\n",
        "            decoded_sentence += ' ' + sampled_token\n",
        "\n",
        "        # exit condition: either hit max length or find stop word.\n",
        "        if (sampled_token == 'eostok' or len(decoded_sentence.split()) >= (MAX_TITLE_LEN-1)):\n",
        "            stop_condition = True\n",
        "\n",
        "        # update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # update internal states\n",
        "        e_h, e_c = h, c\n",
        "\n",
        "    return decoded_sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SIhUu2dz6MN"
      },
      "source": [
        "### Seq2Abstract and Seq2Title Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QveGVxz9z6MN"
      },
      "outputs": [],
      "source": [
        "# Convert the abstract token id sequence back to string representation\n",
        "def seq2abstract(input_seq):\n",
        "    newString = ''\n",
        "    for i in input_seq:\n",
        "        if (i != 0):\n",
        "            newString = newString + reverse_source_word_index[i] + ' '\n",
        "    return newString\n",
        "\n",
        "# Convert the title token id sequence back to string representation\n",
        "def seq2title(input_seq):\n",
        "    newString = ''\n",
        "    for i in input_seq:\n",
        "        if (i != 0 and i != target_word_index['sostok']) and (i != target_word_index['eostok']):\n",
        "            newString = newString + re.sub('<[^>]+>', '', reverse_target_word_index[i]) + ' '\n",
        "    return newString"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LVgjjUYz6MN"
      },
      "source": [
        "## Testing Model\n",
        "### BLEU and ROUGE Evaluation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORcCKvAsz6MO"
      },
      "outputs": [],
      "source": [
        "rouge_metric = Rouge()\n",
        "\n",
        "# Evaluation Metric Function; F1 Score with ROUGE and BLEU\n",
        "def evaluate_title(predicted_ttl, original_ttl):\n",
        "    # Note: prediction and original titles both need to be string sentences\n",
        "    f1_score = 0.0\n",
        "    # get BLEU score\n",
        "    bleu_result = (sentence_bleu(hypothesis=predicted_ttl, references=[original_ttl], smooth_method='exp'))\n",
        "    bleu_score = bleu_result.score/100 # sacreBLEU gives the score in percent\n",
        "\n",
        "    # get ROUGE scores\n",
        "    rouge_result = rouge_metric.get_scores(hyps=predicted_ttl, refs=original_ttl)\n",
        "    rouge_l_score = rouge_result[0][\"rouge-l\"][\"f\"]\n",
        "    rouge_1_score = rouge_result[0][\"rouge-1\"][\"f\"]\n",
        "    rouge_2_score = rouge_result[0][\"rouge-2\"][\"f\"]\n",
        "\n",
        "    # check if bleu or rouge-l score are 0\n",
        "    if (bleu_score == 0.0) or (rouge_l_score == 0.0):\n",
        "        # if so, f1 score is 0\n",
        "        f1_score = 0.0\n",
        "    else:\n",
        "        f1_score = (2.0 * bleu_score * rouge_l_score) / (bleu_score + rouge_l_score)\n",
        "        \n",
        "\n",
        "    # get scores\n",
        "    return (f1_score, bleu_score, rouge_l_score, rouge_1_score, rouge_2_score)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKIXRxX3z6MP"
      },
      "source": [
        "### Print Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGDBU65gz6MP"
      },
      "outputs": [],
      "source": [
        "# Keep track of total F1 score to average at the end\n",
        "total_f1 = 0.0\n",
        "total_bleu = 0.0\n",
        "total_rouge_l = 0.0\n",
        "total_rouge_1 = 0.0\n",
        "total_rouge_2 = 0.0\n",
        "\n",
        "\n",
        "# results_df = pd.DataFrame()\n",
        "# results_df['original_abstract'] = abs_te.tolist()\n",
        "# results_df['original_title'] = ttl_te.tolist()\n",
        "# results_df['original_title'] = results_df['original_title'].apply(lambda row: seq2title(row))\n",
        "# results_df['predicted_title'] = results_df['original_abstract'].apply(lambda row: decode_sequence(np.array(row).reshape(1, MAX_ABSTRACT_LEN)))\n",
        "# results_df['f1_scores'] = results_df.apply(lambda row: evaluate_title(row['predicted_title'], row['original_title']), axis=1)\n",
        "\n",
        "\n",
        "# Loop over all test set data and calculate F1 score for each prediction\n",
        "for i in range(1000):\n",
        "    if i % 10 == 0: print(f\"i = {i}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    original_title = seq2title(ttl_te[i])\n",
        "\n",
        "    prediction_title = decode_sequence(abs_te[i].reshape(1, MAX_ABSTRACT_LEN))\n",
        "    # print(\"Abstract:\", seq2abstract(abs_te[i]))\n",
        "    # print(\"Original Title:\", original_title)\n",
        "    # print(\"Predicted Title:\", prediction_title)\n",
        "\n",
        "\n",
        "    # Skip bad predictions\n",
        "    if prediction_title == \"\" or prediction_title == None: \n",
        "        print(\"Skipping bad prediction...\")\n",
        "        continue\n",
        "\n",
        "\n",
        "\n",
        "    # Generate F1 score for current prediction\n",
        "    f1_score, bleu_score, rouge_l_score, rouge_1_score, rouge_2_score = evaluate_title(prediction_title, original_title)\n",
        "\n",
        "\n",
        "    # Add to total F1\n",
        "    total_f1 += f1_score\n",
        "    total_bleu += bleu_score\n",
        "    total_rouge_l += rouge_l_score\n",
        "    total_rouge_1 += rouge_1_score\n",
        "    total_rouge_2 += rouge_2_score\n",
        "\n",
        "#   if i <= 10:\n",
        "#     print(\"Abstract:\", seq2abstract(abs_te[i]))\n",
        "#     print(\"Original Title:\", original_title)\n",
        "#     print(\"Predicted Title:\", prediction_title)\n",
        "#     print(\"F1 Score:\", f1_score)\n",
        "#     print(\"\\n\")\n",
        "\n",
        "\n",
        "# results_df.to_csv(\"outputs/results.csv\")\n",
        "avg_f1 = total_f1\n",
        "avg_bleu = total_bleu\n",
        "avg_rouge_l = total_rouge_l\n",
        "avg_rouge_1 = total_rouge_1\n",
        "avg_rouge_2 = total_rouge_2\n",
        "\n",
        "print(\"Avg F1 Score:\", avg_f1)\n",
        "print(\"Avg BLEU Score:\", avg_bleu)\n",
        "print(\"Avg ROUGE-L Score:\", avg_rouge_l)\n",
        "print(\"Avg ROUGE-1 Score:\", avg_rouge_1)\n",
        "print(\"Avg ROUGE-2 Score:\", avg_rouge_2)\n",
        "\n",
        "\n",
        "  # Write individual predictions to csv\n",
        "  # flag = \"a\"\n",
        "  # if not os.path.exists(f\"output/{MODEL_CONDITION}.csv\"): flag = \"w\"\n",
        "      \n",
        "  # with open(f\"output/{MODEL_CONDITION}.csv\", flag) as csv_file:\n",
        "  #     csv_writer = csv.writer(csv_file)\n",
        "  #     if flag == \"w\": csv_writer.writerow([\"F1 Score\", \"Original Title\", \"Predicted Title\"])\n",
        "  #     csv_writer.writerow([f1_score, original_title, prediction_title])\n",
        "\n",
        "# Find average F1 score for entire test set (total/len)\n",
        "# avg_f1 = total_f1/len(abs_te)\n",
        "# print(f\"Average F1 score: {avg_f1}\")\n",
        "\n",
        "# # Write results to csv\n",
        "# flag = \"a\"\n",
        "# if not os.path.exists(\"output/model_data.csv\"): flag = \"w\"\n",
        "    \n",
        "# with open(\"output/model_data.csv\", flag) as csv_file:\n",
        "#     csv_writer = csv.writer(csv_file)\n",
        "#     if flag == \"w\": csv_writer.writerow([\"Model Type\", \"Model Condition\", \"Average F1 Score\"])\n",
        "#     csv_writer.writerow([\"LSTM\", MODEL_CONDITION, avg_f1])\n",
        "\n",
        "\n",
        "# Google colab code\n",
        "# Download files\n",
        "# files.download(\"output/model_data.csv\")\n",
        "# files.download(f\"output/{MODEL_CONDITION}.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkZnnlCEBrBF"
      },
      "outputs": [],
      "source": [
        "cleaned_data.to_csv(\"output/cleaned_data.csv\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1 (default, Mar  9 2022, 16:29:28) \n[Clang 13.0.0 (clang-1300.0.27.3)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "55827584c9f2e18e7af3c3bb18834a7a9055c57ce49ebfcd02e392412ec100af"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}