{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvoGHJJvOyAe"
      },
      "source": [
        "# New York Times News Summarization\n",
        "Sirut Buasai, sbausai2@wpi.edu <br>\n",
        "Jason Dykstra, jpdykstra@wpi.edu <br>\n",
        "Adam Yang ayang@wpi.edu\n",
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "emBF0izoOyAo",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Data Processing\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Text Processing\n",
        "import nltk\n",
        "import re\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import pos_tag_sents\n",
        "\n",
        "# Model Building\n",
        "import tensorflow as tf\n",
        "from keras.utils import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Google Colab\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "\n",
        "# Downloads\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "NER = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B73993JWevBU"
      },
      "source": [
        "## Global Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntkhyPUaevBU"
      },
      "outputs": [],
      "source": [
        "MAX_ABSTRACT_LEN = 30\n",
        "MAX_TITLE_LEN = 12\n",
        "LATENT_DIMENSION = 300 \n",
        "EMBEDDING_DIMENSION = 100 \n",
        "EPOCH = 10\n",
        "BATCH_SIZE = 128\n",
        "DATA_SET_RATIO = 0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEs_PaTROyAs"
      },
      "source": [
        "## Data Pre-processing\n",
        "### Initial Data Inspection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEDTd5yNOyAu"
      },
      "outputs": [],
      "source": [
        "raw_data_path = 'NYT_Dataset.csv'\n",
        "raw_data = pd.read_csv(raw_data_path)\n",
        "raw_data = raw_data[:int(DATA_SET_RATIO*len(raw_data))]\n",
        "print(f'dataframe shape: {raw_data.shape}')\n",
        "raw_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yP0xgBIqOyAv"
      },
      "source": [
        "### Clean Data\n",
        "#### Part of Speech Processing Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lt2rCg-K7bn-"
      },
      "outputs": [],
      "source": [
        "def pos_processing(abstract_sentences):\n",
        "  abstract_tokens = pd.Series(abstract_sentences.apply(lambda sentence: sentence.split()))\n",
        "\n",
        "  # identify parts-of-speech; pos_tag requires list of tokens\n",
        "  abstract_pos = pd.Series(pos_tag_sents(abstract_tokens))\n",
        "\n",
        "  # create attention mask(s) using parts-of-speech\n",
        "  nounsList = ['NN', 'NNS', 'NNP', 'NNPS'] # Nouns\n",
        "  verbsList = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VPZ'] # Verbs\n",
        "  adjList = ['JJ', 'JJR', 'JJS'] # Adjectives\n",
        "  noun_attn_mask = abstract_pos.apply(lambda words: [1 if pair[1] in nounsList else 0 for pair in words])\n",
        "  verb_attn_mask = abstract_pos.apply(lambda words: [1 if pair[1] in verbsList else 0 for pair in words])\n",
        "  adj_attn_mask = abstract_pos.apply(lambda words: [1 if pair[1] in adjList else 0 for pair in words])\n",
        "\n",
        "  return noun_attn_mask, verb_attn_mask, adj_attn_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRRQSO8eevBY"
      },
      "source": [
        "#### NER Processing Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eN1mcbw8rbj"
      },
      "outputs": [],
      "source": [
        "def create_ne_attn_mask(entities):\n",
        "  # extract named entities\n",
        "  named_entities = entities.ents\n",
        "  # initialize attention mask\n",
        "  attn_mask = [0 for i in range(len(entities))]\n",
        "\n",
        "  # assign 1's to positions in attn mask corresponding to named-entity positions\n",
        "  for entity in named_entities:\n",
        "      attn_mask[entity.start: entity.end] = [1 for i in range(entity.start,entity.end)]\n",
        "\n",
        "  return attn_mask\n",
        "\n",
        "def ner_processing(abstract_sentences):\n",
        "  # identify entities for each abstract\n",
        "  # NOTE: this is supposedly faster if we use nlp.pipe, but we can look into that tomorrow ...\n",
        "  abs_entities = pd.Series(abstract_sentences.apply(lambda sentence: NER(sentence)))\n",
        "\n",
        "  # extract the original sentences, but now with split contractions\n",
        "  original_sentence = pd.Series(abs_entities.apply(lambda entities: ' '.join([token.orth_ for token in entities])))\n",
        "\n",
        "  # create attention mask(s) using the NE's\n",
        "  ne_attention_mask = abs_entities.apply(lambda entities: create_ne_attn_mask(entities))\n",
        "\n",
        "\n",
        "  return original_sentence, ne_attention_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPpI_mowevBZ"
      },
      "source": [
        "#### Clean Text Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "dvJXzEeFOyAw",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def preprocess_data(df):\n",
        "  # drop unnecessary columns\n",
        "  unused_columns = ['Unnamed: 0', 'Date', 'ID', 'topic', 'keywords']\n",
        "  df.drop(unused_columns, axis=1, inplace=True, errors='ignore')\n",
        "\n",
        "  # drop dupliates and nan rows\n",
        "  df.drop_duplicates(subset=['abstract'], inplace=True)\n",
        "  df.reset_index(drop=True, inplace=True)\n",
        "  df.dropna(axis=0, inplace=True)\n",
        "  df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  # convert texts to lowercase\n",
        "  df['abstract'] = df['abstract'].str.lower()\n",
        "  df['title'] = df['title'].str.lower()\n",
        "\n",
        "  # strip special characters\n",
        "  df['cleaned_title'] = df['title'].apply(lambda string: re.sub(\"[^a-zA-Z0-9 ]+\", '', string))\n",
        "  df['cleaned_abstract'] = df['abstract'].apply(lambda string: re.sub(\"[^a-zA-Z0-9 ]+\", '', string))\n",
        "\n",
        "  # convert string to list\n",
        "  df['cleaned_title'] = df['cleaned_title'].apply(lambda string: string.split())\n",
        "  df['cleaned_abstract'] = df['cleaned_abstract'].apply(lambda string: string.split())\n",
        "\n",
        "  # remove stopwords\n",
        "  stop_words = stopwords.words('english')\n",
        "  df['cleaned_title'] = df['cleaned_title'].apply(lambda sentence: [word for word in sentence if word not in (stop_words)])\n",
        "  df['cleaned_abstract'] = df['cleaned_abstract'].apply(lambda sentence: [word for word in sentence if word not in (stop_words)])\n",
        "\n",
        "  # convert list back to string\n",
        "  df['cleaned_title'] = df['cleaned_title'].apply(lambda sentence: \" \".join(word for word in sentence))\n",
        "  df['cleaned_abstract'] = df['cleaned_abstract'].apply(lambda sentence: \" \".join(word for word in sentence))\n",
        "\n",
        "  # ATTENTION MASK GENERATION NEEDS TO BE LAST PART OF PROCESSING FUNCTION\n",
        "  # identify named-entities; NER requires string sentences\n",
        "  df['cleaned_abstract'], df['ner_attn_mask'] = ner_processing(df['cleaned_abstract'])\n",
        "\n",
        "  # generate noun, verb, adj attention mask from parts-of-speech; requires string sentences\n",
        "  df['noun_attn_mask'], df['verb_attn_mask'], df['adj_attn_mask'] = pos_processing(df['cleaned_abstract'])\n",
        "    \n",
        "  # add start and end tokens for title\n",
        "  df['cleaned_title'] = df['cleaned_title'].apply(lambda text: 'sostok ' + text + ' eostok')\n",
        "\n",
        "  return df\n",
        "\n",
        "cleaned_data = preprocess_data(raw_data)\n",
        "print(f'dataframe shape: {cleaned_data.shape}')\n",
        "cleaned_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(5):\n",
        "  print(\"Abstract:\", cleaned_data['cleaned_abstract'][i])\n",
        "  print(\"Title:\", cleaned_data['cleaned_title'][i])\n",
        "  print(\"\\n\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Insepct String Length Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "abstract_word_count = []\n",
        "title_word_count = []\n",
        "\n",
        "# populate the lists with sentence lengths\n",
        "for i in cleaned_data['cleaned_abstract']:\n",
        "  abstract_word_count.append(len(i.split()))\n",
        "\n",
        "for i in cleaned_data['cleaned_title']:\n",
        "  title_word_count.append(len(i.split()))\n",
        "\n",
        "length_df = pd.DataFrame({'abstract':abstract_word_count, 'title':title_word_count})\n",
        "\n",
        "length_df.hist(bins = 30)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAM-o54M0IAS"
      },
      "source": [
        "### Function for Attention Mask Error Checking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gg-ohbwpgAbv"
      },
      "outputs": [],
      "source": [
        "def attn_mask_error_checking(abstract_text_series, attn_mask_series):\n",
        "  text_list = []\n",
        "  text_len_list = []\n",
        "  text_split_list = []\n",
        "  mask_len_list = []\n",
        "  mask_list = []\n",
        "  # get length of string in text\n",
        "  for row in abstract_text_series:\n",
        "    text_list.append(row)\n",
        "    text_split_list.append(row.split())\n",
        "    text_len_list.append(len(row.split()))\n",
        "  # get length of attention mask\n",
        "  for row in attn_mask_series:\n",
        "    mask_list.append(row)\n",
        "    mask_len_list.append(len(row))\n",
        "    \n",
        "  failed_text_split_list = []\n",
        "  failed_text_list = []\n",
        "  failed_text_len_list = []\n",
        "  failed_mask_list = []\n",
        "  failed_mask_len_list = []\n",
        "  error_count = 0\n",
        "  # compare the length of text with the length of attention mask\n",
        "  for text_len, text_split, text, mask_len, mask in zip(text_len_list, text_split_list, text_list, mask_len_list, mask_list):\n",
        "    if text_len != mask_len:\n",
        "      print(\"length of text is not the same as length of mask\")\n",
        "      failed_text_split_list.append(text_split)\n",
        "      failed_text_list.append(text)\n",
        "      failed_mask_list.append(mask)\n",
        "      failed_mask_len_list.append(mask_len)\n",
        "      failed_text_len_list.append(text_len)\n",
        "      error_count += 1\n",
        "        \n",
        "  return (failed_text_split_list, failed_text_list, failed_mask_list, failed_text_len_list, failed_mask_len_list, error_count)\n",
        "\n",
        "for mask in ['ner_attn_mask', 'noun_attn_mask', 'verb_attn_mask', 'adj_attn_mask']:\n",
        "  tsl, tl, ml, tll, mll, error = attn_mask_error_checking(cleaned_data['cleaned_abstract'], cleaned_data[mask])\n",
        "  print(f\"checking {mask}\\n\\terror: {error}\")\n",
        "  if error > 0:\n",
        "    print(f\"\\t{mask} text and mask lists\")\n",
        "    # print(f\"\\t{tsl})\n",
        "    # print(f\"\\t{tl})\n",
        "    # print(f\"\\t{ml})\n",
        "    # print(f\"\\t{tll})\n",
        "    # print(f\"\\t{mll})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jc7jseOdOyA2"
      },
      "source": [
        "### Split Training and Testing Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRIL1YDyOyA3"
      },
      "outputs": [],
      "source": [
        "abs_tr, abs_te, ttl_tr, ttl_te = train_test_split(\n",
        "  np.array(cleaned_data['cleaned_abstract']),\n",
        "  np.array(cleaned_data['cleaned_title']),\n",
        "  test_size=0.1,\n",
        "  random_state=0,\n",
        "  shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvUKlvl1OyA4"
      },
      "source": [
        "### Tokenize Title and Abstracts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3dOe-5fOyA5"
      },
      "outputs": [],
      "source": [
        "def tokenize_text(train_set, test_set, string_length, rare_word_threshold):\n",
        "  # find the number of rarewords\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(list(train_set))\n",
        "  rare_word_count = 0\n",
        "  total_word_count = 0\n",
        "  for _,freq in tokenizer.word_counts.items():\n",
        "    total_word_count = total_word_count + 1\n",
        "    if (freq < rare_word_threshold):\n",
        "      rare_word_count += 1\n",
        "    \n",
        "  # prepare a tokenizer for reviews on training data\n",
        "  tokenizer = Tokenizer(num_words=total_word_count-rare_word_count)\n",
        "  tokenizer.fit_on_texts(list(train_set))\n",
        "\n",
        "  # convert text sequences into integer sequences\n",
        "  train_set = tokenizer.texts_to_sequences(train_set) \n",
        "  test_set = tokenizer.texts_to_sequences(test_set)\n",
        "\n",
        "  # padding zero up to maximum length\n",
        "  train_set = pad_sequences(train_set, maxlen=string_length, padding='post') \n",
        "  test_set = pad_sequences(test_set, maxlen=string_length, padding='post')\n",
        "\n",
        "  # get vector size\n",
        "  vocab_size = tokenizer.num_words+1\n",
        "\n",
        "  return tokenizer, train_set, test_set, vocab_size\n",
        "\n",
        "abs_tokenizer, abs_tr, abs_te, abs_size = tokenize_text(abs_tr, abs_te, MAX_ABSTRACT_LEN, 3)\n",
        "ttl_tokenizer, ttl_tr, ttl_te, ttl_size = tokenize_text(ttl_tr, ttl_te, MAX_TITLE_LEN, 6)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Remove Empty Texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_empty_text(abstract, title):\n",
        "  indices = []\n",
        "  for i in range(len(ttl_tr)):\n",
        "      cnt = 0\n",
        "      for j in ttl_tr[i]:\n",
        "          if j!=0:\n",
        "              cnt=cnt+1\n",
        "      if (cnt == 2):\n",
        "          indices.append(i)\n",
        "\n",
        "  abstract = np.delete(abs_tr, indices, axis=0)\n",
        "  title = np.delete(ttl_tr, indices, axis=0)\n",
        "  print(f\"removed {len(indices)} empty texts.\")\n",
        "\n",
        "  return abstract, title\n",
        "\n",
        "abs_tr, ttl_tr = remove_empty_text(abs_tr, ttl_tr)\n",
        "abs_te, ttl_te = remove_empty_text(abs_te, ttl_te)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TaFaS9iOyA5"
      },
      "source": [
        "## Build LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tf.keras.backend.clear_session() \n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = tf.keras.layers.Input(shape=(MAX_ABSTRACT_LEN,))\n",
        "\n",
        "# embedding layer\n",
        "enc_emb =  tf.keras.layers.Embedding(abs_size, \n",
        "                                     EMBEDDING_DIMENSION, \n",
        "                                     trainable=True)(encoder_inputs)\n",
        "\n",
        "# encoder lstm 1\n",
        "encoder_lstm1 = tf.keras.layers.LSTM(LATENT_DIMENSION, \n",
        "                                     return_sequences=True, \n",
        "                                     return_state=True, \n",
        "                                     dropout=0.4, \n",
        "                                     recurrent_dropout=0.4)\n",
        "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
        "\n",
        "# encoder lstm 2\n",
        "encoder_lstm2 = tf.keras.layers.LSTM(LATENT_DIMENSION,\n",
        "                                     return_sequences=True,\n",
        "                                     return_state=True,\n",
        "                                     dropout=0.4,\n",
        "                                     recurrent_dropout=0.4)\n",
        "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
        "\n",
        "# encoder lstm 3\n",
        "encoder_lstm3 = tf.keras.layers.LSTM(LATENT_DIMENSION, \n",
        "                                     return_state=True, \n",
        "                                     return_sequences=True,\n",
        "                                     dropout=0.4,\n",
        "                                     recurrent_dropout=0.4)\n",
        "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
        "\n",
        "# set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = tf.keras.layers.Input(shape=(None,))\n",
        "\n",
        "# embedding layer\n",
        "dec_emb_layer = tf.keras.layers.Embedding(ttl_size, \n",
        "                                          EMBEDDING_DIMENSION, \n",
        "                                          trainable=True)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "decoder_lstm = tf.keras.layers.LSTM(LATENT_DIMENSION, \n",
        "                                    return_sequences=True, \n",
        "                                    return_state=True,\n",
        "                                    dropout=0.4,\n",
        "                                    recurrent_dropout=0.2)\n",
        "decoder_outputs, decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,\n",
        "                                                                     initial_state=[state_h, state_c])\n",
        "\n",
        "# NOTE: taking attention layer out for now since tutorial uses custom attention layer\n",
        "# attention layer\n",
        "attn_layer = tf.keras.layers.Attention()\n",
        "attn_out = attn_layer([decoder_outputs, encoder_outputs])\n",
        "\n",
        "# concat attention input and decoder LSTM output\n",
        "decoder_concat_input = tf.keras.layers.Concatenate()([decoder_outputs, attn_out])\n",
        "\n",
        "# dense layer\n",
        "decoder_dense =  tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(ttl_size, activation='softmax'))\n",
        "decoder_outputs = decoder_dense(decoder_concat_input)\n",
        "# decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# define the model \n",
        "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.summary() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2E4k41J3OyA7"
      },
      "source": [
        "### Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTQyN3I7OyA8"
      },
      "outputs": [],
      "source": [
        "# initialize model\n",
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "# train model\n",
        "history = model.fit([abs_tr, ttl_tr[:,:-1]], ttl_tr.reshape(ttl_tr.shape[0], ttl_tr.shape[1], 1)[:,1:],\n",
        "                    epochs=EPOCH,\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    validation_data=([abs_te,ttl_te[:,:-1]], ttl_te.reshape(ttl_te.shape[0], ttl_te.shape[1], 1)[:,1:]))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inspect Training Validation Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Vec2Word Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reverse_target_word_index=ttl_tokenizer.index_word \n",
        "reverse_source_word_index=abs_tokenizer.index_word \n",
        "target_word_index=ttl_tokenizer.word_index"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build Inference Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# encode the input sequence to get the feature vector\n",
        "encoder_model = tf.keras.models.Model(inputs=encoder_inputs,\n",
        "                                      outputs=[encoder_outputs, state_h, state_c])\n",
        "\n",
        "# decoder setup\n",
        "# below tensors will hold the states of the previous time step\n",
        "decoder_state_input_h = tf.keras.layers.Input(shape=(LATENT_DIMENSION,))\n",
        "decoder_state_input_c = tf.keras.layers.Input(shape=(LATENT_DIMENSION,))\n",
        "decoder_hidden_state_input = tf.keras.layers.Input(shape=(MAX_ABSTRACT_LEN, LATENT_DIMENSION))\n",
        "\n",
        "# get the embeddings of the decoder sequence\n",
        "dec_emb2= dec_emb_layer(decoder_inputs) \n",
        "# to predict the next word in the sequence, set the initial states to the states from the previous time step\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
        "\n",
        "# NOTE: taking attention layer out for now since tutorial uses custom attention layer\n",
        "# attention inference\n",
        "attn_out_inf = attn_layer([decoder_outputs2, decoder_hidden_state_input])\n",
        "decoder_inf_concat = tf.keras.layers.Concatenate()([decoder_outputs2, attn_out_inf])\n",
        "\n",
        "# a dense softmax layer to generate prob dist. over the target vocabulary\n",
        "decoder_outputs2 = decoder_dense(decoder_inf_concat) \n",
        "# decoder_outputs2 = decoder_dense(decoder_outputs2) \n",
        "\n",
        "# final decoder model\n",
        "decoder_model = tf.keras.models.Model([decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
        "                                      [decoder_outputs2] + [state_h2, state_c2])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Decode Inference Output Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def decode_sequence(input_seq):\n",
        "  # encode the input as state vectors.\n",
        "  e_out, e_h, e_c = encoder_model.predict(input_seq, verbose=0)\n",
        "  \n",
        "  # generate empty target sequence of length 1.\n",
        "  target_seq = np.zeros((1,1))\n",
        "  \n",
        "  # populate the first word of target sequence with the start word.\n",
        "  target_seq[0, 0] = target_word_index['sostok']\n",
        "\n",
        "  stop_condition = False\n",
        "  decoded_sentence = ''\n",
        "  while not stop_condition:\n",
        "  \n",
        "    output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c], verbose=0)\n",
        "\n",
        "    # sample a token\n",
        "    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "    sampled_token = reverse_target_word_index[sampled_token_index]\n",
        "    \n",
        "    if (sampled_token != 'eostok'):\n",
        "      decoded_sentence += ' ' + sampled_token\n",
        "\n",
        "    # exit condition: either hit max length or find stop word.\n",
        "    if (sampled_token == 'eostok' or len(decoded_sentence.split()) >= (MAX_TITLE_LEN-1)):\n",
        "      stop_condition = True\n",
        "\n",
        "    # update the target sequence (of length 1).\n",
        "    target_seq = np.zeros((1,1))\n",
        "    target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "    # update internal states\n",
        "    e_h, e_c = h, c\n",
        "\n",
        "  return decoded_sentence"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Seq2Abstract and Seq2Title Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def seq2abstract(input_seq):\n",
        "    newString = ''\n",
        "    for i in input_seq:\n",
        "      if (i != 0):\n",
        "        newString = newString + reverse_source_word_index[i] + ' '\n",
        "    return newString\n",
        "\n",
        "def seq2title(input_seq):\n",
        "    newString = ''\n",
        "    for i in input_seq:\n",
        "      if (i != 0 and i != target_word_index['sostok']) and (i != target_word_index['eostok']):\n",
        "        newString = newString + reverse_target_word_index[i] + ' '\n",
        "    return newString"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(len(abs_te)):\n",
        "  print(\"Abstract:\", seq2abstract(abs_te[i]))\n",
        "  print(\"Original Title:\", seq2title(ttl_te[i]))\n",
        "  print(\"Predicted Title:\", decode_sequence(abs_te[i].reshape(1, MAX_ABSTRACT_LEN)))\n",
        "  print(\"\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.14 64-bit ('3.9.14')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.14"
    },
    "vscode": {
      "interpreter": {
        "hash": "d50662f09ec6209d433ff0029bf9cc367be01420c8c5568a1e0b3ae42a6d5264"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
